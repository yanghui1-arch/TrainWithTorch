{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer Module\n",
    "- setting: hyperparameter<br/>\n",
    "- setup_seed: reproduce the same result<br/>\n",
    "- Trainer: create a trainer to train<br/>\n",
    "    - `init` <br/>\n",
    "        1. device\n",
    "        2. model\n",
    "        3. data\n",
    "            - train\n",
    "            - validition\n",
    "            - test\n",
    "        4. optimizer\n",
    "        5. other parameters\n",
    "            - current steps\n",
    "            - iteration steps: int. When current steps // iteration steps, start evaluating.\n",
    "            - current minimum loss: float. If loss is less than current minimum loss, the loss should be record and save the model checkpoint this time.\n",
    "            - val_loss: loss on validation data\n",
    "            - test_loss: loss on test data\n",
    "    - `train`<br/>\n",
    "        Model enters train mode and then initialize some parameters such as training_steps, process_bar and time_stamp that will be used in the next procedure.Finally start trianing by epochs nums. --> _run_epoch()\n",
    "    - `_run_epoch` <br/>\n",
    "        DataLoader break the data into many mini-batches so that program can process data with batches quickly.Specifically, we are training on many batches of data in one epoch. --> _run_batch(). <br/><br/>\n",
    "        When after processing data of one batch, we can do something like evaluating or saving and other operations.<br/>\n",
    "        - When we would like to evaluate checkpoint, model has to enter eval mode. And then start evaluating. --> _run_eval() \n",
    "        - When we would like to save checkpoint we have to compare loss and current minimum loss. --> _save_ckp()\n",
    "        - After all operations done, remember model must re-enter train mode. And then we can calculate how much time we have spent since last evaluated.\n",
    "    <br/>\n",
    "    - `_run_batch`<br/>\n",
    "        Calculate logits and loss. Then set gradient to zero before backward. Finally loss.backward and parameter update.\n",
    "    - `_run_eval`<br/>\n",
    "        Note that **MUST BE IN NO GRAD BLOCK**. Use dataloader to calculate loss and something metric such as accuracy, f1, recall and precise.\n",
    "    - `_save_ckp`<br/>\n",
    "        Save model in certain path.\n",
    "- loader_train(): load trainer class<br/>\n",
    "\n",
    "# Dataset Module\n",
    "\n",
    "- `init`<br/>\n",
    "    1. input\n",
    "    2. output\n",
    "    3. tokenizer\n",
    "    4. other parameters\n",
    "    Note input and output will be probably preprocessed.\n",
    "- `getitem`:Overwrite<br/>\n",
    "    Return input[index] and output[index] with passing away an `index`.\n",
    "- `len`:Overwrite<br/>\n",
    "    Return the number of samples. \n",
    "- `collate_fn`<br/>\n",
    "    Devide dataset to many batches calling DataLoader.We usually do some tokenization and transform data type to torch.Tensor \n",
    "- `load_data`<br/>\n",
    "    Load data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
